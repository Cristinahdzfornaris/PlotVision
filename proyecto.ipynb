{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80091888",
   "metadata": {},
   "source": [
    "### Entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aa4a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2. Funci贸n de carga con DATA AUGMENTATION para CINE\n",
    "def get_dataloaders(data_dir, batch_size=32, is_grayscale=False):\n",
    "    lista_transformaciones = [transforms.Resize((224, 224))]\n",
    "    \n",
    "    if is_grayscale:\n",
    "        lista_transformaciones.append(transforms.Grayscale(num_output_channels=3))\n",
    "    \n",
    "    # A帽adimos transformaciones que imitan el cine (sombras, giros, ruido)\n",
    "    lista_transformaciones.extend([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15), # La gente no siempre tiene la cara recta\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3), # Sombras de pel铆cula\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    data_transforms = transforms.Compose(lista_transformaciones)\n",
    "    dataset = datasets.ImageFolder(root=data_dir, transform=data_transforms)\n",
    "    \n",
    "    # Dividimos en entrenamiento (80%) y validaci贸n (20%) para ver m茅tricas reales\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    return train_loader, val_loader, dataset.classes\n",
    "\n",
    "# 3. Funci贸n de Evaluaci贸n (M茅tricas de verdad)\n",
    "def evaluate_model(model, loader, classes, phase_name):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"\\n---  MTRICAS FINALES: {phase_name} ---\")\n",
    "    print(f\"Accuracy: {acc:.4f} | F1-Score: {f1:.4f}\")\n",
    "    print(classification_report(y_true, y_pred, target_names=classes))\n",
    "    \n",
    "    # Matriz de Confusi贸n\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=classes, yticklabels=classes, cmap='Blues')\n",
    "    plt.title(f'Matriz de Confusi贸n - {phase_name}')\n",
    "    plt.show()\n",
    "\n",
    "# 4. Funci贸n de Entrenamiento Actualizada\n",
    "def train_model(model, train_loader, val_loader, epochs, learning_rate, phase_name, classes):\n",
    "    print(f\"\\n=== Iniciando fase: {phase_name} ===\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss, all_preds, all_labels = 0.0, [], []\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "        epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {running_loss/len(train_loader):.4f} - Acc: {epoch_acc:.4f}\")\n",
    "    \n",
    "    # Al final de la fase, evaluar con el set de validaci贸n\n",
    "    evaluate_model(model, val_loader, classes, phase_name)\n",
    "    return model\n",
    "\n",
    "# 1. Funci贸n para calcular pesos autom谩ticamente\n",
    "def get_class_weights(dataset):\n",
    "    import numpy as np\n",
    "    labels = [sample[1] for sample in dataset.samples]\n",
    "    classes = np.unique(labels)\n",
    "    \n",
    "    # 1. Calculamos los pesos balanceados normales\n",
    "    weights = compute_class_weight(class_weight='balanced', classes=classes, y=labels)\n",
    "    \n",
    "    # 2. SUAVIZADO: Aplicamos la ra铆z cuadrada (Square Root)\n",
    "    # Esto reduce la \"agresividad\" del castigo hacia las clases mayoritarias\n",
    "    weights = np.sqrt(weights)\n",
    "    \n",
    "    # 3. NORMALIZACIN (Opcional): Que el peso de \"Happy\" y \"Neutral\" no baje de 0.8\n",
    "    # para que no pierdan tanta importancia\n",
    "    weights = np.maximum(weights, 0.8)\n",
    "    \n",
    "    return torch.tensor(weights, dtype=torch.float).to(device)\n",
    "\n",
    "# 2. Funci贸n de carga de datos (modificada para devolver el dataset completo)\n",
    "def get_dataloaders(data_dir, batch_size=32, is_grayscale=False):\n",
    "    lista_transformaciones = [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3)\n",
    "    ]\n",
    "    \n",
    "    if is_grayscale:\n",
    "        lista_transformaciones.insert(1, transforms.Grayscale(num_output_channels=3))\n",
    "    \n",
    "    lista_transformaciones.extend([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    data_transforms = transforms.Compose(lista_transformaciones)\n",
    "    full_dataset = datasets.ImageFolder(root=data_dir, transform=data_transforms)\n",
    "    \n",
    "    # Calcular pesos antes de dividir\n",
    "    weights = get_class_weights(full_dataset)\n",
    "    \n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, full_dataset.classes, weights\n",
    "\n",
    "# 3. Funci贸n de entrenamiento con pesaje de p茅rdida\n",
    "def train_model_weighted(model, train_loader, val_loader, weights, epochs, lr, phase_name, classes):\n",
    "    print(f\"\\n=== Fase: {phase_name} (Usando Pesos de Clase) ===\")\n",
    "    \n",
    "    # AQU EST EL CAMBIO: Pasamos los pesos al criterio\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights, label_smoothing=0.1)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Scheduler: Baja el learning rate si el modelo deja de mejorar\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        preds_list, labels_list = [], []\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            preds_list.extend(preds.cpu().numpy())\n",
    "            labels_list.extend(labels.cpu().numpy())\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = accuracy_score(labels_list, preds_list)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.4f}\")\n",
    "        scheduler.step(epoch_loss)\n",
    "\n",
    "    return model\n",
    "\n",
    "# ==========================================\n",
    "# FLUJO PRINCIPAL\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # --- FASE 1: FER2013 ---\n",
    "    train_f, val_f, cls_f, w_fer = get_dataloaders(\"./Data/fer2013/train\", is_grayscale=True)\n",
    "    print(f\"Pesos calculados para FER: {w_fer}\")\n",
    "    \n",
    "    model = models.resnet18(pretrained=True)\n",
    "    model.fc = nn.Linear(model.fc.in_features, len(cls_f))\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Entrenamiento\n",
    "    model = train_model_weighted(model, train_f, val_f, w_fer, 5, 0.001, \"FER2013\", cls_f)\n",
    "    \n",
    "    # --- AGREGADO: LLAMADA A EVALUACIN PARA FER2013 ---\n",
    "    evaluate_model(model, val_f, cls_f, \"FER2013 Final\")\n",
    "\n",
    "    # --- FASE 2: AFFECTNET ---\n",
    "    train_a, val_a, cls_a, w_aff = get_dataloaders(\"./Data/affectnet/train\", is_grayscale=False)\n",
    "    print(f\"Pesos calculados para AffectNet: {w_aff}\")\n",
    "    \n",
    "    # Cirug铆a de Red (Cambiamos la salida de 7 a 8 clases)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, len(cls_a))\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Entrenamiento con pesos\n",
    "    model = train_model_weighted(model, train_a, val_a, w_aff, 10, 0.0001, \"AffectNet\", cls_a)\n",
    "    \n",
    "   \n",
    "    evaluate_model(model, val_a, cls_a, \"AffectNet Final\")\n",
    "    \n",
    "    # Guardar el modelo final\n",
    "    torch.save(model.state_dict(), \"modelo_pesado_final.pth\")\n",
    "    print(\"\\n隆Todo listo! El modelo ha sido entrenado, evaluado y guardado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39449e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from insightface.app import FaceAnalysis\n",
    "from scipy.spatial.distance import cosine\n",
    "import math\n",
    "import json\n",
    "from collections import deque, Counter\n",
    "\n",
    "# ==========================================\n",
    "# 1. GESTOR DE IDENTIDAD ROBUSTO (Centroid Tracking)\n",
    "# ==========================================\n",
    "class IdentityManager:\n",
    "    def __init__(self, threshold=0.55):\n",
    "        self.face_bank = [] \n",
    "        self.id_counter = 0\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def get_identity(self, embedding):\n",
    "        if embedding is None: return \"Unknown\"\n",
    "        best_match_idx = -1\n",
    "        min_dist = float('inf')\n",
    "\n",
    "        for i, person in enumerate(self.face_bank):\n",
    "            dist = cosine(person[\"embedding\"], embedding)\n",
    "            if dist < self.threshold and dist < min_dist:\n",
    "                min_dist = dist\n",
    "                best_match_idx = i\n",
    "\n",
    "        if best_match_idx != -1:\n",
    "            # Actualizaci贸n din谩mica del rostro (Promedio ponderado)\n",
    "            old_emb = self.face_bank[best_match_idx][\"embedding\"]\n",
    "            count = self.face_bank[best_match_idx][\"count\"]\n",
    "            new_average = (old_emb * count + embedding) / (count + 1)\n",
    "            self.face_bank[best_match_idx][\"embedding\"] = new_average\n",
    "            self.face_bank[best_match_idx][\"count\"] += 1\n",
    "            return self.face_bank[best_match_idx][\"id\"]\n",
    "        else:\n",
    "            new_id = f\"Personaje_{self.id_counter}\"\n",
    "            self.face_bank.append({\"id\": new_id, \"embedding\": embedding, \"count\": 1})\n",
    "            self.id_counter += 1\n",
    "            return new_id\n",
    "\n",
    "# ==========================================\n",
    "# 2. ANALIZADOR DE PELCULAS CON FILTRO TEMPORAL\n",
    "# ==========================================\n",
    "class MovieAnalyzer:\n",
    "    def __init__(self, emotion_model_path):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Cargar Modelo \n",
    "        self.emotion_model = models.resnet18()\n",
    "        self.emotion_model.fc = nn.Linear(self.emotion_model.fc.in_features, 8) \n",
    "        self.emotion_model.load_state_dict(torch.load(emotion_model_path, map_location=self.device))\n",
    "        self.emotion_model.to(self.device).eval()\n",
    "        \n",
    "        # Detectores\n",
    "        self.detector = FaceAnalysis(name='buffalo_l', providers=['CPUExecutionProvider'])\n",
    "        self.detector.prepare(ctx_id=0, det_size=(640, 640))\n",
    "        self.identity_manager = IdentityManager(threshold=0.55)\n",
    "        \n",
    "        \n",
    "        self.emotion_buffers = {}\n",
    "        \n",
    "        self.history = {\n",
    "            \"metadata\": {\"video\": \"\", \"total_frames\": 0},\n",
    "            \"personajes\": {},\n",
    "            \"bitacora\": [],\n",
    "            \"interacciones\": []\n",
    "        }\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def predict_emotion(self, face_crop, kps=None):\n",
    "        if face_crop.size == 0: return \"neutral\"\n",
    "        \n",
    "        img = Image.fromarray(cv2.cvtColor(face_crop, cv2.COLOR_BGR2RGB))\n",
    "        img = self.transform(img).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.emotion_model(img)\n",
    "           \n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            probs[0][4] *= 1.4  \n",
    "            probs[0][7] *= 1.2  \n",
    "            \n",
    "            confianza, predicted = torch.max(probs, 1)\n",
    "            \n",
    "        labels = {0:'anger', 1:'contempt', 2:'disgust', 3:'fear', 4:'happy', 5:'neutral', 6:'sad', 7:'surprise'}\n",
    "        pred_label = labels[predicted.item()]\n",
    "\n",
    "        \n",
    "        if kps is not None:\n",
    "            boca_izq = kps[3]\n",
    "            boca_der = kps[4]\n",
    "            ojo_izq = kps[0]\n",
    "            ojo_der = kps[1]\n",
    "            \n",
    "            dist_boca = np.linalg.norm(boca_izq - boca_der)\n",
    "            dist_ojos = np.linalg.norm(ojo_izq - ojo_der)\n",
    "            \n",
    "            \n",
    "            ratio_sonrisa = dist_boca / dist_ojos\n",
    "            \n",
    "            if ratio_sonrisa > 0.85: \n",
    "                \n",
    "                if pred_label in ['sad', 'anger', 'neutral']:\n",
    "                    return \"happy\"\n",
    "\n",
    "        \n",
    "        if pred_label == 'happy' and confianza.item() > 0.25:\n",
    "            return \"happy\"\n",
    "        elif confianza.item() < 0.40:\n",
    "            return \"neutral\"\n",
    "            \n",
    "        return pred_label\n",
    "\n",
    "    def get_stable_emotion(self, char_id, raw_emotion):\n",
    "        \"\"\"Aplica la l贸gica de promedio de 10 frames (Moda)\"\"\"\n",
    "        if char_id not in self.emotion_buffers:\n",
    "            self.emotion_buffers[char_id] = deque(maxlen=10)\n",
    "        \n",
    "        self.emotion_buffers[char_id].append(raw_emotion)\n",
    "        \n",
    "        # La emoci贸n estable es la que m谩s se repite en el buffer (Moda)\n",
    "        counts = Counter(self.emotion_buffers[char_id])\n",
    "        stable_emotion = counts.most_common(1)[0][0]\n",
    "        return stable_emotion\n",
    "\n",
    "    def is_looking(self, p1, p2):\n",
    "        p1_nose = p1[\"kps\"][2]\n",
    "        p2_center = p2[\"center\"]\n",
    "        v_target = np.array([p2_center[0] - p1_nose[0], p2_center[1] - p1_nose[1]])\n",
    "        eye_center = (p1[\"kps\"][0] + p1[\"kps\"][1]) / 2\n",
    "        v_gaze = p1_nose - eye_center\n",
    "        unit_target = v_target / (np.linalg.norm(v_target) + 1e-6)\n",
    "        unit_gaze = v_gaze / (np.linalg.norm(v_gaze) + 1e-6)\n",
    "        angle = math.degrees(math.acos(np.clip(np.dot(unit_target, unit_gaze), -1.0, 1.0)))\n",
    "        return angle < 35\n",
    "\n",
    "    def process_video(self, video_path):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        self.history[\"metadata\"][\"video\"] = video_path\n",
    "        \n",
    "        frame_idx = 0\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break\n",
    "            \n",
    "            # Procesamos frames intercalados para eficiencia\n",
    "            if frame_idx % 5 == 0:\n",
    "                segundo = round(frame_idx / fps, 2)\n",
    "                faces = self.detector.get(frame)\n",
    "                current_faces_data = []\n",
    "\n",
    "                for face in faces:\n",
    "                    char_id = self.identity_manager.get_identity(face.normed_embedding)\n",
    "                    \n",
    "                    # 1. Obtener emoci贸n cruda\n",
    "                    x1, y1, x2, y2 = face.bbox.astype(int)\n",
    "                    crop = frame[max(0,y1):y2, max(0,x1):x2]\n",
    "                    raw_emotion = self.predict_emotion(crop)\n",
    "                    \n",
    "                    # 2. ESTABILIZACIN: Promedio de 10 frames\n",
    "                    # emotion = self.get_stable_emotion(char_id, raw_emotion)\n",
    "                    emotion = self.predict_emotion(crop, kps=face.kps)\n",
    "                    # Guardar en Bit谩cora y Estad铆sticas\n",
    "                    self.history[\"bitacora\"].append({\"segundo\": segundo, \"id\": char_id, \"emocion\": emotion})\n",
    "                    \n",
    "                    if char_id not in self.history[\"personajes\"]:\n",
    "                        self.history[\"personajes\"][char_id] = {\"conteo_emociones\": {}}\n",
    "                    self.history[\"personajes\"][char_id][\"conteo_emociones\"][emotion] = \\\n",
    "                        self.history[\"personajes\"][char_id][\"conteo_emociones\"].get(emotion, 0) + 1\n",
    "\n",
    "                    current_faces_data.append({\n",
    "                        \"id\": char_id, \"center\": ((x1+x2)//2, (y1+y2)//2), \n",
    "                        \"bbox\": (x1, y1, x2, y2), \"emotion\": emotion, \"kps\": face.kps\n",
    "                    })\n",
    "\n",
    "                # Analizar Interacciones (Miradas)\n",
    "                for p1 in current_faces_data:\n",
    "                    # Dibujar\n",
    "                    cv2.rectangle(frame, (p1[\"bbox\"][0], p1[\"bbox\"][1]), (p1[\"bbox\"][2], p1[\"bbox\"][3]), (0, 255, 0), 2)\n",
    "                    cv2.putText(frame, f\"{p1['id']}: {p1['emotion']}\", (p1[\"bbox\"][0], p1[\"bbox\"][1]-10), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "\n",
    "                    for p2 in current_faces_data:\n",
    "                        if p1[\"id\"] != p2[\"id\"] and self.is_looking(p1, p2):\n",
    "                            self.history[\"interacciones\"].append({\n",
    "                                \"segundo\": segundo, \"de\": p1[\"id\"], \"a\": p2[\"id\"], \"emocion\": p1[\"emotion\"]\n",
    "                            })\n",
    "                            cv2.line(frame, (int(p1[\"kps\"][2][0]), int(p1[\"kps\"][2][1])), p2[\"center\"], (0, 255, 255), 2)\n",
    "\n",
    "                cv2.imshow('Analisis Estabilizado', frame)\n",
    "                \n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'): break\n",
    "            frame_idx += 1\n",
    "            \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        return self.history\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Usa el modelo entrenado con pesos (el que mejor F1-Score tuvo)\n",
    "    analyzer = MovieAnalyzer(\"modelo_pesado_final.pth\")\n",
    "    resultados = analyzer.process_video(\"teaser.mp4\")\n",
    "    \n",
    "    with open(\"historia_completa.json\", \"w\") as f:\n",
    "        json.dump(resultados, f, indent=4)\n",
    "    print(\"An谩lisis finalizado con suavizado temporal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de60158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generar_grafo_relaciones(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        datos = json.load(f)\n",
    "\n",
    "    G = nx.DiGraph() # Grafo dirigido (A mira a B)\n",
    "\n",
    "    # Procesar interacciones\n",
    "    for inter in datos[\"interacciones\"]:\n",
    "        u, v = inter[\"de\"], inter[\"a\"]\n",
    "        emocion = inter[\"emocion\"]\n",
    "        \n",
    "        if G.has_edge(u, v):\n",
    "            G[u][v]['weight'] += 1\n",
    "            G[u][v]['emociones'].append(emocion)\n",
    "        else:\n",
    "            G.add_edge(u, v, weight=1, emociones=[emocion])\n",
    "\n",
    "    # Dibujar\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    pos = nx.spring_layout(G)\n",
    "    \n",
    "    weights = [G[u][v]['weight'] * 0.5 for u, v in G.edges()]\n",
    "    \n",
    "    nx.draw(G, pos, with_labels=True, node_color='skyblue', \n",
    "            node_size=2000, width=weights, edge_color='gray', arrowsize=20)\n",
    "    \n",
    "    plt.title(\"Grafo de Relaciones de Miradas en la Pel铆cula\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "generar_grafo_relaciones(\"historia_completa.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
